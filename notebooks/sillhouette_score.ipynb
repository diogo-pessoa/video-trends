{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Sillhoute Score Analysis for optmizing KMeans\n",
    "---\n",
    "Diogo Pessoa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Uncomment this Section if running locally or working on this notebook and loading dataset directly here.\"\"\"\n",
    "# import os\n",
    "# import sys\n",
    "#\n",
    "# from dotenv import load_dotenv\n",
    "#\n",
    "# # Load environment variables from a .env file\n",
    "# load_dotenv()\n",
    "# images_path = os.getenv('IMAGES_PATH')\n",
    "# data_dir = os.getenv('DATA_COLLECTION_DIR')\n",
    "#\n",
    "# # Loading local helper modules\n",
    "# module_path = os.path.abspath(os.path.join('..'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "# # data_collection\n",
    "# %run './data_collection.ipynb'\n",
    "# # features engineering\n",
    "# %run './feature_engineering.ipynb'\n",
    "# \"\"\"\n",
    "# Local Dataset from local files (download if not present)\n",
    "#  :returns sampled_df_with_added_features: DataFrame, sampled_df_with_added_features_indexed: DataFrame\n",
    "# \"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['start_station_id_index', 'day_period_index']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# # Scaling the features\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# # Combine the VectorAssembler and StandardScaler into a Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# You can now define a pipeline that includes both the assembler and the scaler\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fit and transform the DataFrame using the defined pipeline\n",
    "sampled_df_scaled = pipeline.fit(sampled_df_with_added_features_indexed).transform(\n",
    "    sampled_df_with_added_features_indexed)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "k_values = list(range(2, 11))\n",
    "\n",
    "# Initialize an empty list to store silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# Iterate over values of k\n",
    "for k in k_values:\n",
    "    # Initialize KMeans with the specified number of clusters (k) and a seed for reproducibility\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"scaled_features\")\n",
    "\n",
    "    # Fit the model to the data\n",
    "    model = kmeans.fit(sampled_df_scaled)\n",
    "\n",
    "    # Transform the dataset to include cluster predictions\n",
    "    predictions = model.transform(sampled_df_scaled)\n",
    "\n",
    "    # Evaluate the model\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhouette_scores.append(silhouette)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, silhouette_scores, color='orange')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette score vs. Number of clusters (k)')\n",
    "plt.xticks(np.arange(min(k_values), max(k_values) + 1, 1.0))\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(images_path, 'silhouette_score_day_period_start_stations_vs_number_of_clusters.png'))\n",
    "plt.show()\n",
    "# Output the optimal k based on silhouette score\n",
    "optimal_k = k_values[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f\"The optimal number of clusters k is: {optimal_k}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
