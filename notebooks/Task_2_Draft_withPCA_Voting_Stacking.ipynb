{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Tuning the best performing models\n",
    "---\n",
    "\n",
    "After initial review of Bagging, Boosting and Stacking. I'll now focus on the best 3 three models and apply PCA, GridSearch seeking to improve accuracy.\n",
    "\n",
    "Diogo Pessoa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "combined_data = pd.read_csv('combined_data.csv').set_index('video_id')\n",
    "label = combined_data['label']\n",
    "features = combined_data.drop(['label'], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features.values, label.values, test_size=0.2, random_state=0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train_sc = sc.fit_transform(x_train)\n",
    "x_test_sc = sc.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO - List the best models\n",
    "\"\"\"\n",
    "\n",
    "RandomForestRegressor 0.19906876900824988 0.9399100491646171 0.9399541017739272\n",
    "- Performed quite well, however we'll skip this model since it was used on notebook provided.\n",
    "  - Keeping it in table for comparison.\n",
    "Baseline: LinearRegression 0.446259142254775 0.8652943399834946 0.8653391206880249\n",
    "\n",
    "\n",
    "Bagging performing well. \n",
    "Ensemble Method 'mean squared error', 'r2 score', 'explained variance score'\n",
    "BaggingRegressor 0.1989198958725586 0.9399549873005636 0.9400026998167074\n",
    "VotingRegressor 0.2962409649534499 0.9105781127388121 0.9114122683042218 (Interesting as we can combine the other models under review. Yet, it's not the best performing model.)\n",
    "\n",
    "\n",
    "Boosting -  keeing at least one for comparison\n",
    "\n",
    "Ensemble Method 'mean squared error', 'r2 score', 'explained variance score'\n",
    "GradientBoostingRegressor 0.3799829950579041 0.8853001422319302 0.885301770354857\n",
    "\n",
    "Stacking - In its own notebook\n",
    "\n",
    "|Ensemble|mean squared error| |r2 score| |explained variance score|\n",
    "|Stacking Regressor|0.4227565962173764 | 0.8718890758241176 | 0.8719155515438354|\n",
    "\n",
    "'Estimators: RidgeCV, LassoCV, KNeighborsRegressor, GradientBoostingRegressor'\n",
    "\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=34)\n",
    "x_train_Trans=pca.fit_transform(x_train)\n",
    "x_test_Trans=pca.transform(x_test)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.enemble import VotingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "\n",
    "# default values\n",
    "n_estimators=140\n",
    "rd_state=42\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "gbr = GradientBoostingRegressor(random_state=rd_state)\n",
    "\n",
    "bagging_regressor = BaggingRegressor(random_state=rd_state, n_estimators=n_estimators)\n",
    "voting_reg = VotingRegressor(estimators=[('gb', grad_boosting_regressor), ('rf', random_forest_regressor), ('lr', linear_regr), ('ada_b', ada_boosting_regressor), ('bagging_r', bagging_regressor)]) # TODO\n",
    "\n",
    "# Only tune the max depth of the trees in the RF hyperparameter.\n",
    "grid = GridSearchCV(estimator=[voting_reg,bagging_regressor],\n",
    "                    scoring=['neg_mean_squared_error','r2','explained_variance'], n_jobs=-1)\n",
    "grid.fit(x_train_Trans, y_train)\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "grid.best_params_\n",
    "print(f\"GridSearchCV took {duration:.2f} seconds.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "y_pred = grid.predict(x_test_Trans)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_two_score = r2_score(y_test, y_pred)\n",
    "ex_variance_score = explained_variance_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error on Test Set: {mse}')\n",
    "print('|Ensemble|mean squared error|', '|r2 score|', '|explained variance score|')\n",
    "print(f'|Bagging & Voting Regressor|{mse} | {r_two_score} | {ex_variance_score}|')\n",
    "\n",
    "# BaggingRegressor 0.1989198958725586 0.9399549873005636 0.9400026998167074 (before optmization)\n",
    "# VotingRegressor 0.2962409649534499 0.9105781127388121 0.9114122683042218  (before optmization)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define the base models, with opmitized parameters\n",
    "\n",
    "base_models = [\n",
    "    ('ridge', RidgeCV()),\n",
    "    ('lasso', LassoCV(random_state=42)),\n",
    "    ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean'))\n",
    "]\n",
    "# Define the meta-model\n",
    "meta_model = GradientBoostingRegressor(random_state=42)\n",
    "# Create the stacking model\n",
    "stacked_model = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "\n",
    "scores = cross_val_score(stacked_model, x_train_Trans, y_train,  scoring=['neg_mean_squared_error','r2','explained_variance'], cv=kf)\n",
    "\n",
    "# Convert scores to positive (since they are negative mean squared errors)\n",
    "mse_scores = -scores\n",
    "\n",
    "# Calculate RMSE for each fold\n",
    "rmse_scores = np.sqrt(mse_scores)\n",
    "\n",
    "print(f\"RMSE scores for each fold: {rmse_scores}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO Predict and print metrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "y_pred = grid.predict(x_test_Trans)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_two_score = r2_score(y_test, y_pred)\n",
    "ex_variance_score = explained_variance_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error on Test Set: {mse}')\n",
    "print('|Ensemble|mean squared error|', '|r2 score|', '|explained variance score|')\n",
    "print(f'|Stacking Regressor|{mse} | {r_two_score} | {ex_variance_score}|')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
