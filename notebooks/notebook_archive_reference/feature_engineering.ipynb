{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Feature Engineering\n",
    "---\n",
    "Diogo Pessoa\n",
    "\n",
    "## Objective\n",
    "\n",
    "Call data_collection notebook to load the dataset and apply feature engineering to the dataset.\n",
    "\n",
    "### Adding new features\n",
    "\n",
    "As discussed in the [Technical Report](Reports/out/Technical_Report_Diogo_Pessoa.pdf).\n",
    "I'll add a categorical field for time of day and day of week. Later I'll use these fields to analyze the most popular stations by time of day and day of week.\n",
    "For both Categorical fields the idea is to simplify the data and make it easier to analyze. While also labeling trips and opening an avenue for further exploration in regression models\n",
    "* Time of day: Morning, Afternoon, Evening, Night\n",
    "* Day of week: Workday, non-Working"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "sampled_df = divvy_df.sample(withReplacement=False, fraction=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T21:32:58.226138Z",
     "start_time": "2024-02-06T21:32:58.194515Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # removing duplicates and null values from ride_id and station Ids\n",
    "sampled_df = sampled_df.dropDuplicates(subset=['ride_id']).dropna(subset=['ride_id'])\n",
    "# # Get unique bike stations, removing rows with null values for station Ids\n",
    "from divvy_bike_share_data_analysis.bike_stations import get_unique_bike_stations_ids\n",
    "\n",
    "sampled_df = sampled_df.dropna(subset=['start_station_id', 'end_station_id'])\n",
    "sampled_df.show(10)\n",
    "bike_stations = get_unique_bike_stations_ids(sampled_df.select(['start_station_id',\n",
    "                                                                'start_station_name',\n",
    "                                                                'end_station_id',\n",
    "                                                                'end_station_name']))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying unified Station Names by ID back to sampled dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check started_at and ended_at columns for null values\n",
    "sampled_df = sampled_df.dropna(subset=['started_at', 'ended_at'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, hour, dayofweek\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# TODO refactoring to optmized queries and avoid duplicated columns after joins.\n",
    "# [Note to self] check bike_stations.get_unique_ids function for similar approach.\n",
    "\n",
    "# Prepare start and end stations DataFrames from bike_stations\n",
    "start_stations = bike_stations.selectExpr(\"station_id as start_station_id\", \"station_name as new_start_station_name\")\n",
    "end_stations = bike_stations.selectExpr(\"station_id as end_station_id\", \"station_name as new_end_station_name\")\n",
    "\n",
    "# Drop existing name columns in sampled_df before joining\n",
    "sampled_df = sampled_df.drop(\"start_station_name\", \"end_station_name\")\n",
    "\n",
    "# Join with start_stations to add new_start_station_name\n",
    "sampled_df = sampled_df.join(start_stations, on=\"start_station_id\", how=\"left\").withColumnRenamed(\"new_start_station_name\", \"start_station_name\")\n",
    "\n",
    "# Join with end_stations to add new_end_station_name\n",
    "sampled_df = sampled_df.join(end_stations, on=\"end_station_id\", how=\"left\").withColumnRenamed(\"new_end_station_name\", \"end_station_name\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "\"\"\" rideable_type - 0: classic_bike, 1: docked_bike, 2: electric_bike\n",
    "We'll discard the docked_bike type as it's not relevant for the predictive analysis of the number of bikes needed at each station at different times of the day.\"\"\"\n",
    "\n",
    "sampled_df = sampled_df.filter(sampled_df['rideable_type'] != 'docked_bike')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T19:50:21.082292Z",
     "start_time": "2024-02-06T19:50:21.025771Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding a categorical field for time of day and day of week.\n",
    "from divvy_bike_share_data_analysis.bike_stations import categorize_time_of_day, categorize_day_of_week\n",
    "\n",
    "week_day_udf = udf(categorize_day_of_week, StringType())\n",
    "time_of_day_udf = udf(categorize_time_of_day, StringType())\n",
    "sampled_df_with_added_features = sampled_df.withColumn('day_period', time_of_day_udf(hour('started_at'))).withColumn(\n",
    "    'week_day', week_day_udf(dayofweek('started_at')))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# Assuming your initial DataFrame is named sampled_df_with_added_features and you want to transform it using the defined StringIndexers\n",
    "\n",
    "indexer_start_station_id = StringIndexer(inputCol=\"start_station_id\", outputCol=\"start_station_id_index\")\n",
    "indexer_end_station_id = StringIndexer(inputCol=\"end_station_id\", outputCol=\"end_station_id_index\")\n",
    "indexer_member_casual = StringIndexer(inputCol=\"member_casual\", outputCol=\"member_casual_index\")\n",
    "indexer_day_period = StringIndexer(inputCol=\"day_period\", outputCol=\"day_period_index\")\n",
    "indexer_week_day = StringIndexer(inputCol=\"week_day\", outputCol=\"week_day_index\")\n",
    "indexer_rideable_type = StringIndexer(inputCol=\"rideable_type\", outputCol=\"rideable_type_index\")\n",
    "\n",
    "# Combine all indexers into a Pipeline for streamlined processing\n",
    "pipeline = Pipeline(stages=[\n",
    "    indexer_start_station_id,\n",
    "    indexer_end_station_id,\n",
    "    indexer_member_casual,\n",
    "    indexer_day_period,\n",
    "    indexer_week_day,\n",
    "    indexer_rideable_type\n",
    "])\n",
    "\n",
    "# Fit and transform the DataFrame using the pipeline\n",
    "sampled_df_with_added_features_indexed = pipeline.fit(sampled_df_with_added_features).transform(sampled_df_with_added_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'loaded sampled_df_with_added_features with {sampled_df_with_added_features.count()} \\n and columns {sampled_df_with_added_features.columns}')\n",
    "\n",
    "print(f'loaded sampled_df_with_added_features_indexed with {sampled_df_with_added_features_indexed.count()} \\n and columns {sampled_df_with_added_features_indexed.columns}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
